{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "2025-01-31 09:02:24.588920: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import itertools as it\n",
    "import random as rd\n",
    "import scipy.sparse as sp\n",
    "import scipy.stats as st\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import scipy.spatial.distance as dist\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "import math\n",
    "import urllib\n",
    "import gzip\n",
    "import colorsys\n",
    "import pickle as pk\n",
    "import os.path\n",
    "from collections import defaultdict, Counter, ChainMap\n",
    "import umap\n",
    "import time\n",
    "# import statsmodels.sandbox.stats.multicomp as mc\n",
    "from sklearn.preprocessing import normalize\n",
    "import plotly.graph_objects as go\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rnd_walk_matrix(G, r, a):\n",
    "\n",
    "    n = G.number_of_nodes()\n",
    "    A = nx.adjacency_matrix(G,sorted(G.nodes()))\n",
    "\n",
    "    factor = float((1-a)/n)\n",
    "\n",
    "    E = np.multiply(factor,np.ones([n,n]))              # prepare 2nd scaling term\n",
    "    A_tele = np.multiply(a,A) + E  \n",
    "\n",
    "#     print(A_tele)\n",
    "    M = normalize(A_tele, norm='l1', axis=0)                                 # column wise normalized MArkov matrix\n",
    "\n",
    "    # mixture of Markov chains\n",
    "    del A_tele\n",
    "    del E\n",
    "\n",
    "    U = np.identity(n,dtype=int) \n",
    "    H = (1-r)*M\n",
    "    H1 = np.subtract(U,H)\n",
    "    del U\n",
    "    del M\n",
    "    del H    \n",
    "\n",
    "    W = r*np.linalg.inv(H1)   \n",
    "\n",
    "    return W\n",
    "\n",
    "\n",
    "def fisher_test(sig_level,sampleset,d_sample_attributes,d_attributes_sample,background):\n",
    "    #####\n",
    "    # IDENTIFY ALL TERMS TO BE TESTED IN SAMPLE\n",
    "    #####\n",
    "    # take only these neighboring genes that come with a disease annotation\n",
    "    sample_overlap = set(sampleset) & set(d_sample_attributes.keys())\n",
    "    # sample_overlap = sampleset\n",
    "    \n",
    "    l_terms = []\n",
    "    for gene in sample_overlap:\n",
    "        for x in d_sample_attributes[gene]:\n",
    "            l_terms.append(x)\n",
    "        \n",
    "    # print('tests :%s ' %len(l_terms))\n",
    "    # print('set of tests :%s ' %len(set(l_terms)))\n",
    "    set_terms = set(l_terms)\n",
    "    number_of_tests = len(set_terms)\n",
    "\n",
    "    # make a dictionary of all GO terms with their p-values \n",
    "    d_term_p = {}\n",
    "\n",
    "    for term in set_terms:\n",
    "        attributeset = set(d_attributes_sample[term])\n",
    "\n",
    "        ab = len(sample_overlap.intersection(attributeset))\n",
    "        amb = len(sample_overlap.difference(attributeset))\n",
    "        bma = len(attributeset.difference(sample_overlap))\n",
    "#         backg = background - len(set(sample_overlap) | set(attributeset))\n",
    "        backg = background - ab - amb - bma\n",
    "\n",
    "        # pval = pvalue(ab , amb, bma,backg).right_tail\n",
    "        oddsratio, pval = st.fisher_exact([[ab, amb], [bma, backg]],alternative='greater')\n",
    "        d_term_p[term] = pval * number_of_tests\n",
    "#         print(pval)\n",
    "        # f.write('%s,%s,%s,%s,%.6f\\n' %(ab,amb,bma,backg,pval))\n",
    "    # f.close()\n",
    "    return d_term_p \n",
    "\t\n",
    "\n",
    "##################################################################\n",
    "##################################################################\n",
    "\n",
    "def color_creator(n):\n",
    "\n",
    "    colors = [colorsys.hsv_to_rgb(1.0/n*x,1,1) for x in range(n)]\n",
    "    color_list = []\n",
    "    for c in colors:\n",
    "        cc = [int(y*255) for y in c]\n",
    "        color_list.append('#%02x%02x%02x' % (cc[0],cc[1],cc[2]))\n",
    "        \n",
    "    return color_list\n",
    "\n",
    "\n",
    "\n",
    "def make_different_hexcolors(N):\n",
    "    N_colors = N\n",
    "\n",
    "    # packing criterium\n",
    "    d_min_c = 9./5*33554432/np.pi/N\n",
    "\n",
    "    d_mindistance = d_min_c**(2./3.)\n",
    "    # print(d_mindistance)\n",
    "\n",
    "    # init_color = f\"#{''.join(np.random.choice(list('0123456789abcdef'), size=6))}\" \n",
    "    init_col = [rd.randint(0, 255),rd.randint(0, 255),rd.randint(0, 255)]\n",
    "    # print(init_col)\n",
    "    l_colors = []\n",
    "    l_colors.append(init_col)\n",
    "    # for n in range(1):\n",
    "    while len(l_colors) < N_colors:\n",
    "        rd_col = [rd.randint(0, 255),rd.randint(0, 255),rd.randint(0, 255)]\n",
    "        # print(rd_col)\n",
    "        l_dists = []\n",
    "        for c in l_colors:\n",
    "            d = (rd_col[0] - c[0])**2 + (rd_col[1] - c[1])**2 + (rd_col[2] - c[2])**2\n",
    "\n",
    "            l_dists.append(d)\n",
    "        if min(l_dists)>d_mindistance:\n",
    "            l_colors.append(rd_col)\n",
    "\n",
    "    l_hexcols = [rgb2hex(rgb[0],rgb[1],rgb[2]) for rgb in l_colors]\n",
    "    return l_hexcols\n",
    "\n",
    "\n",
    "def Ginfo(G):\n",
    "    info = []\n",
    "    info.append(f\"Graph Name: {G.name}\")\n",
    "    info.append(f\"Graph Type: {type(G).__name__}\")\n",
    "    info.append(f\"Number of Nodes: {G.number_of_nodes()}\")\n",
    "    info.append(f\"Number of Edges: {G.number_of_edges()}\")\n",
    "    # info.append(f\"Density: {round(100. * nx.density(G), 3)} %\")\n",
    "\n",
    "    # if nx.is_weighted(G):\n",
    "    #     info.append(f\"Weighted: Yes\")\n",
    "        \n",
    "    #     # Calculate the average degree considering edge weights for weighted graphs\n",
    "    #     avg_weighted_degree = sum(weight for u, v, weight in G.edges(data=True)) / G.number_of_nodes()\n",
    "    #     info.append(f\"Average Weighted Degree: {round(avg_weighted_degree, 2)}\")\n",
    "    # else:\n",
    "    #     info.append(f\"Weighted: No\")\n",
    "        \n",
    "    #     # Calculate the average degree as usual for unweighted graphs\n",
    "    #     avg_degree = sum(dict(G.degree()).values()) / G.number_of_nodes()\n",
    "    #     info.append(f\"Average Degree: {round(avg_degree, 2)}\")\n",
    "\n",
    "    info.append(f\"Connected: {'Yes' if nx.is_connected(G) else 'No'}\")\n",
    "    num_components = nx.number_connected_components(G)\n",
    "    info.append(f\"Number of Connected Components: {num_components}\")\n",
    "\n",
    "    info.append(f\"Directed: {'Yes' if G.is_directed() else 'No'}\")\n",
    "    \n",
    "    return \"\\n\".join(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def download_mapping_file(url, filename):\n",
    "    urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "def parse_mapping_file(filename):\n",
    "    mapping = {}\n",
    "    with gzip.open(filename, 'rt') as file:\n",
    "        for line in file:\n",
    "            if line.startswith('#'):  # Skip header and comments\n",
    "                continue\n",
    "            fields = line.strip().split('\\t')\n",
    "            # print(fields)\n",
    "            if fields[0] == '9606':\n",
    "                gene_symbol = fields[2]\n",
    "                entrez_id = fields[1]\n",
    "                # print(entrez_id)\n",
    "                # ensembl_id = fields[5]\n",
    "                # Additional mapping field\n",
    "                gene_name = fields[10]\n",
    "\n",
    "                # try: \n",
    "                #     ent_int = int(entrez_id)\n",
    "                #     gene_name = fields[10]\n",
    "                # except:\n",
    "                #     gene_name = entrez_id\n",
    "\n",
    "                if gene_symbol not in mapping:\n",
    "                    mapping[gene_symbol] = {\n",
    "                        'entrez_id': entrez_id,\n",
    "                        # 'ensembl_id': ensembl_id,\n",
    "                        'gene_name': gene_name\n",
    "                    }\n",
    "    return mapping\n",
    "\n",
    "\n",
    "def convert_symbols_to_entrez(gene_symbols, mapping):\n",
    "    entrez_ids = []\n",
    "    for symbol in gene_symbols:\n",
    "        if symbol in mapping:\n",
    "            entrez_ids.append(mapping[symbol]['entrez_id'])\n",
    "    return entrez_ids\n",
    "\n",
    "# def convert_symbols_to_ensembl(gene_symbols, mapping):\n",
    "#     ensembl_ids = []\n",
    "#     for symbol in gene_symbols:\n",
    "#         if symbol in mapping:\n",
    "#             ensembl_ids.append(mapping[symbol]['ensembl_id'])\n",
    "#     return ensembl_ids\n",
    "\n",
    "# Download Gene Info file (example using Homo sapiens)\n",
    "filename = 'Homo_sapiens.gene_info.gz'\n",
    "url = 'ftp://ftp.ncbi.nlm.nih.gov/gene/DATA/GENE_INFO/Mammalia/' + filename\n",
    "\n",
    "# download_mapping_file(url, filename)\n",
    "\n",
    "# Parse mapping file\n",
    "mapping = parse_mapping_file(filename)\n",
    "\n",
    "# Generate a dictionary of all existing gene symbols and their corresponding Entrez IDs and Ensemble IDs\n",
    "all_gene_symbols = list(mapping.keys())\n",
    "all_entrez_ids = [mapping[symbol]['entrez_id'] for symbol in all_gene_symbols]\n",
    "# all_ensembl_ids = [mapping[symbol]['ensembl_id'] for symbol in all_gene_symbols]\n",
    "symbol_to_entrez = dict(zip(all_gene_symbols, all_entrez_ids))\n",
    "# symbol_to_ensembl = dict(zip(all_gene_symbols, all_ensembl_ids))\n",
    "entrez_to_symbols = {v: k for k, v in symbol_to_entrez.items()}\n",
    "# ensembl_to_symbols = {v: k for k, v in symbol_to_ensembl.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full PPI\n",
      "Graph Name: \n",
      "Graph Type: Graph\n",
      "Number of Nodes: 18068\n",
      "Number of Edges: 306914\n",
      "Connected: No\n",
      "Number of Connected Components: 2\n",
      "Directed: No\n",
      "\n",
      "lcc PPI\n",
      "Graph Name: \n",
      "Graph Type: Graph\n",
      "Number of Nodes: 18064\n",
      "Number of Edges: 306911\n",
      "Connected: Yes\n",
      "Number of Connected Components: 1\n",
      "Directed: No\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/fmueller/work/NICE/GitNICE/Data/'\n",
    "\n",
    "\n",
    "print('full PPI')\n",
    "# PPI direct\n",
    "G_ppi = nx.read_edgelist(path + 'PPIs/PPI_physical_elist.csv',data=False, delimiter=',')\n",
    "print(Ginfo(G_ppi))\n",
    "print('\\nlcc PPI')\n",
    "\n",
    "lcc = max(nx.connected_components(G_ppi), key=len)\n",
    "G = nx.subgraph(G_ppi,lcc)\n",
    "\n",
    "print(Ginfo(G))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load annotation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## GO - Functions\n",
    "# d_attributes_sample = pk.load( open(path + \"GO/GOupF2genes.pkl\", \"rb\" ) )\n",
    "# d_sample_attributes = pk.load( open(path + \"GO/gene2GOF_up.pkl\", \"rb\" ) )\n",
    "\n",
    "# GO - compionents\n",
    "d_attributes_sample = pk.load( open(path + \"GO/GOupC2genes.pkl\", \"rb\" ) )\n",
    "d_sample_attributes = pk.load( open(path + \"GO/gene2GOC_up.pkl\", \"rb\" ) )\n",
    "\n",
    "d_go_names = pk.load( open(path + \"GO/d_gonames.pkl\", \"rb\" ) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protein Complex GO Terms:\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Define the GO term for \"protein complex\" (GO:0043234)\n",
    "GO_TERM = \"GO:0043234\"\n",
    "\n",
    "# Use the Gene Ontology API to fetch children of the \"protein complex\" category\n",
    "url = f\"https://www.ebi.ac.uk/QuickGO/services/ontology/go/terms/{GO_TERM}/children\"\n",
    "\n",
    "# Send the request\n",
    "response = requests.get(url, headers={\"Accept\": \"application/json\"})\n",
    "\n",
    "list_GO_protein_complexes = []\n",
    "\n",
    "# Parse and display results\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    children = data[\"results\"][0][\"children\"]\n",
    "    print(\"Protein Complex GO Terms:\")\n",
    "    for term in children:\n",
    "        # print(f\"{term['id']} - {term['name']}\")\n",
    "        list_GO_protein_complexes.append(term)\n",
    "else:\n",
    "    print(\"Error retrieving data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "281"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_GO_protein_complexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GO:1990393\n",
      "3M complex\n",
      "{23363, 83987, 9820}\n",
      "OBSL1\n",
      "CCDC8\n",
      "CUL7\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "print(list_GO_protein_complexes[n]['id'])\n",
    "print(list_GO_protein_complexes[n]['name'])\n",
    "print(d_attributes_sample[list_GO_protein_complexes[n]['id']])\n",
    "\n",
    "for gene in d_attributes_sample[list_GO_protein_complexes[n]['id']]:\n",
    "    print(entrez_to_symbols[str(gene)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def write_go_protein_data(list_GO_protein_complexes, d_attributes_sample, entrez_to_symbols, output_file=\"go_protein_complexes.csv\"):\n",
    "    \"\"\"Writes GO protein complexes and their associated proteins to a CSV file.\"\"\"\n",
    "    \n",
    "    with open(output_file, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write header\n",
    "        writer.writerow([\"GO ID\", \"GO Name\", \"Protein Symbols\"])\n",
    "        \n",
    "        for n in range(len(list_GO_protein_complexes)):\n",
    "            try:\n",
    "                go_id = list_GO_protein_complexes[n]['id']\n",
    "                go_name = list_GO_protein_complexes[n]['name']\n",
    "                proteins = d_attributes_sample[go_id]\n",
    "                \n",
    "                if proteins:  # Ensure there are proteins\n",
    "                    # Convert Entrez IDs to gene symbols\n",
    "                    protein_symbols = [entrez_to_symbols.get(str(p), str(p)) for p in proteins]  # Keep original if not found\n",
    "                    \n",
    "                    # Write row\n",
    "                    writer.writerow([go_id, go_name] + protein_symbols)\n",
    "            except KeyError:\n",
    "                pass  # Skip if GO ID is not found in d_attributes_sample\n",
    "\n",
    "# Example usage\n",
    "write_go_protein_data(list_GO_protein_complexes, d_attributes_sample, entrez_to_symbols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "e774977668b7c0ae8309835a5187aa7fbf7669e7d0bb59755bc63e573643edcd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
